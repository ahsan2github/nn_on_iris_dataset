{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "import sklearn as skl\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n",
      "Number of training saplmes:  120\n"
     ]
    }
   ],
   "source": [
    "# pre process\n",
    "iris = sns.load_dataset(\"iris\");\n",
    "iris.dropna(axis=0, how='any', inplace=True);\n",
    "print(iris.head(5));\n",
    "target_df = pd.get_dummies(iris['species']);\n",
    "iris  = iris.drop(['species'], axis=1);\n",
    "scaler = MinMaxScaler()\n",
    "#iris[iris.columns] = scaler.fit_transform(iris[iris.columns])\n",
    "df = pd.concat([iris, target_df], axis=1)\n",
    "#print(df.head(5))\n",
    "train = df.sample(frac=0.8,random_state=10)\n",
    "test = df.drop(train.index)\n",
    "print(\"Number of training saplmes: \", len(train))\n",
    "outclass = ['setosa', 'versicolor', 'verginica']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0, Train accuracy: 0.68\n",
      "epoch:  100, Train accuracy: 0.97\n",
      "test accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "class parameters():\n",
    "    with tf.variable_scope('weights_biases'):\n",
    "        def __init__(self, n_features, n_hidden_layers, n_hidden_neurons, n_output):\n",
    "            self.n_features = n_features;\n",
    "            self.n_hidden_layers = n_hidden_layers;\n",
    "            self.n_hidden_neurons = n_hidden_neurons;\n",
    "            self.n_output = n_output;\n",
    "            self.bias = []\n",
    "            self.weights = [];\n",
    "            assert (len(n_hidden_neurons) == n_hidden_layers), \\\n",
    "                \"Length of #nerons list must be equal to #hidden layers \";\n",
    "            for i in np.arange(self.n_hidden_layers+1):\n",
    "                if i==0:\n",
    "                    self.weights.append(tf.Variable(np.random.randn(n_features, \\\n",
    "                            n_hidden_neurons[0]), dtype=tf.float32))\n",
    "                elif i==self.n_hidden_layers:\n",
    "                     self.weights.append(tf.Variable(np.random.randn(n_hidden_neurons[i-1],n_output), \\\n",
    "                            dtype=tf.float32))\n",
    "                else:\n",
    "                     self.weights.append(tf.Variable(np.random.randn(n_hidden_neurons[i-1], \\\n",
    "                            n_hidden_neurons[i]), dtype=tf.float32))\n",
    "                    \n",
    "            for i in np.arange(self.n_hidden_layers+1):\n",
    "                if i < self.n_hidden_layers:\n",
    "                    self.bias.append(tf.Variable(tf.zeros((1, n_hidden_neurons[i])), dtype=tf.float32))\n",
    "                else:\n",
    "                    self.bias.append(tf.Variable(tf.zeros((1,n_output)), dtype=tf.float32))\n",
    "\n",
    "\n",
    "\"\"\" Build the model \"\"\"\n",
    "param = parameters(4,1,[4],3);\n",
    "X = tf.placeholder(tf.float32, shape=(None, param.n_features), name='X');\n",
    "y = tf.placeholder(tf.int32, shape=(None, param.n_output), name='y');\n",
    "hidden_one = tf.nn.relu(tf.add(tf.matmul(X, param.weights[0]), param.bias[0]));\n",
    "output_layer = tf.add(tf.matmul(hidden_one, param.weights[1]), param.bias[1]);\n",
    "y_ = tf.nn.softmax(output_layer)\n",
    "\n",
    "\"\"\" Optimize the model \"\"\"\n",
    "xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_layer)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "lrate = 0.1;\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = lrate)\n",
    "training_oprn = optimizer.minimize(loss)\n",
    "\n",
    "\"\"\"check accuracy\"\"\"\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 101\n",
    "batch_size = 12\n",
    "init_op = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(df.shape[0] // batch_size):\n",
    "            X_batch, y_batch = train.iloc[iteration*batch_size:iteration*batch_size+batch_size, 0:4], \\\n",
    "                               train.iloc[iteration*batch_size:iteration*batch_size+batch_size, 4:7]\n",
    "            sess.run(training_oprn, feed_dict={X: X_batch, y: y_batch})\n",
    "        X_batch, y_batch = train.iloc[:, 0:4], train.iloc[:,4:7]     \n",
    "        if epoch % 100 == 0:\n",
    "            print(\"epoch: {0:4d}, Train accuracy: {1:1.2f}\".format(epoch, sess.run(accuracy,feed_dict={X: X_batch, y: y_batch})))\n",
    "    X_batch, y_batch = test.iloc[:, 0:4], test.iloc[:,4:7] \n",
    "    print(\"test accuracy: {0:1.2f}\".format(sess.run(accuracy,feed_dict={X: X_batch, y: y_batch})))\n",
    "    saver.save(sess, \"./iris_model.ckpt\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./iris_model.ckpt\n",
      "Given feature values: \n",
      "   sepal_length  sepal_width  petal_length  petal_width\n",
      "0           5.0          1.0           1.4          0.2\n",
      "corresponding predicted class:  ['setosa']\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  # Restore variables from disk.\n",
    "    saver.restore(sess, \"./iris_model.ckpt\")    \n",
    "    X1 = pd.DataFrame(data={\"sepal_length\": [5.0], \"sepal_width\": [1.0], \"petal_length\": [1.4], \"petal_width\": [0.2]})\n",
    "    # enforce the order of columns otherwise pandas can change the order \n",
    "    X1 = X1[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "    print(\"Given feature values: \")\n",
    "    print(X1);    \n",
    "    #X1[X1.columns] = scaler.fit_transform(X1[X1.columns]);\n",
    "    i = sess.run(tf.cast(tf.argmax(y_,1), tf.int8),feed_dict={X: X1})\n",
    "    outputclass = [outclass[ii] for ii in np.arange(len(i))];\n",
    "    print(\"corresponding predicted class: \", outputclass);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
